{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load train and test data\n",
    "df_train = pd.read_csv(\"avila-tr.txt\", header=None , names=['F1','F2','F3','F4','F5','F6','F7','F8','F9','F10','Class']);\n",
    "df_test = pd.read_csv(\"avila-ts.txt\", header=None , names=['F1','F2','F3','F4','F5','F6','F7','F8','F9','F10','Class']);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>F1</th>\n",
       "      <th>F2</th>\n",
       "      <th>F3</th>\n",
       "      <th>F4</th>\n",
       "      <th>F5</th>\n",
       "      <th>F6</th>\n",
       "      <th>F7</th>\n",
       "      <th>F8</th>\n",
       "      <th>F9</th>\n",
       "      <th>F10</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-3.498799</td>\n",
       "      <td>0.250492</td>\n",
       "      <td>0.232070</td>\n",
       "      <td>1.224178</td>\n",
       "      <td>-4.922215</td>\n",
       "      <td>1.145386</td>\n",
       "      <td>0.182426</td>\n",
       "      <td>-0.165983</td>\n",
       "      <td>-0.123005</td>\n",
       "      <td>1.087144</td>\n",
       "      <td>W</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.204355</td>\n",
       "      <td>-0.354049</td>\n",
       "      <td>0.320980</td>\n",
       "      <td>0.410166</td>\n",
       "      <td>-0.989576</td>\n",
       "      <td>-2.218127</td>\n",
       "      <td>0.220177</td>\n",
       "      <td>0.181844</td>\n",
       "      <td>2.090879</td>\n",
       "      <td>-2.009758</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.759828</td>\n",
       "      <td>-1.304042</td>\n",
       "      <td>-0.023991</td>\n",
       "      <td>-0.973663</td>\n",
       "      <td>-0.006417</td>\n",
       "      <td>-0.349509</td>\n",
       "      <td>-0.421580</td>\n",
       "      <td>-0.450127</td>\n",
       "      <td>0.469443</td>\n",
       "      <td>0.060952</td>\n",
       "      <td>I</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.005490</td>\n",
       "      <td>0.360409</td>\n",
       "      <td>0.281860</td>\n",
       "      <td>-0.213479</td>\n",
       "      <td>-1.168333</td>\n",
       "      <td>-1.013906</td>\n",
       "      <td>-0.346080</td>\n",
       "      <td>1.176165</td>\n",
       "      <td>0.968347</td>\n",
       "      <td>-0.627999</td>\n",
       "      <td>E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.080916</td>\n",
       "      <td>0.101320</td>\n",
       "      <td>0.104040</td>\n",
       "      <td>0.140490</td>\n",
       "      <td>0.261718</td>\n",
       "      <td>0.480988</td>\n",
       "      <td>0.710932</td>\n",
       "      <td>-0.253430</td>\n",
       "      <td>-0.497183</td>\n",
       "      <td>0.155681</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10432</th>\n",
       "      <td>-0.128929</td>\n",
       "      <td>-0.040001</td>\n",
       "      <td>0.057807</td>\n",
       "      <td>0.557894</td>\n",
       "      <td>0.261718</td>\n",
       "      <td>-0.930856</td>\n",
       "      <td>-0.044076</td>\n",
       "      <td>1.158458</td>\n",
       "      <td>2.277968</td>\n",
       "      <td>-0.699884</td>\n",
       "      <td>X</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10433</th>\n",
       "      <td>0.266074</td>\n",
       "      <td>0.556689</td>\n",
       "      <td>-0.020434</td>\n",
       "      <td>0.176624</td>\n",
       "      <td>0.261718</td>\n",
       "      <td>-0.515608</td>\n",
       "      <td>0.597681</td>\n",
       "      <td>0.178349</td>\n",
       "      <td>0.625350</td>\n",
       "      <td>-0.657245</td>\n",
       "      <td>G</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10434</th>\n",
       "      <td>-0.054866</td>\n",
       "      <td>0.580242</td>\n",
       "      <td>0.032912</td>\n",
       "      <td>-0.016668</td>\n",
       "      <td>0.261718</td>\n",
       "      <td>1.519109</td>\n",
       "      <td>0.371178</td>\n",
       "      <td>-0.985508</td>\n",
       "      <td>-0.403638</td>\n",
       "      <td>1.276301</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10435</th>\n",
       "      <td>0.080916</td>\n",
       "      <td>0.588093</td>\n",
       "      <td>0.015130</td>\n",
       "      <td>0.002250</td>\n",
       "      <td>0.261718</td>\n",
       "      <td>-0.930856</td>\n",
       "      <td>-0.270579</td>\n",
       "      <td>0.163807</td>\n",
       "      <td>-0.091823</td>\n",
       "      <td>-0.593329</td>\n",
       "      <td>F</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10436</th>\n",
       "      <td>0.377169</td>\n",
       "      <td>0.014957</td>\n",
       "      <td>0.381439</td>\n",
       "      <td>0.292753</td>\n",
       "      <td>0.261718</td>\n",
       "      <td>-1.470679</td>\n",
       "      <td>-0.006326</td>\n",
       "      <td>-0.494919</td>\n",
       "      <td>-0.247731</td>\n",
       "      <td>-1.212974</td>\n",
       "      <td>H</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10437 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             F1        F2        F3        F4        F5        F6        F7  \\\n",
       "0     -3.498799  0.250492  0.232070  1.224178 -4.922215  1.145386  0.182426   \n",
       "1      0.204355 -0.354049  0.320980  0.410166 -0.989576 -2.218127  0.220177   \n",
       "2      0.759828 -1.304042 -0.023991 -0.973663 -0.006417 -0.349509 -0.421580   \n",
       "3     -0.005490  0.360409  0.281860 -0.213479 -1.168333 -1.013906 -0.346080   \n",
       "4      0.080916  0.101320  0.104040  0.140490  0.261718  0.480988  0.710932   \n",
       "...         ...       ...       ...       ...       ...       ...       ...   \n",
       "10432 -0.128929 -0.040001  0.057807  0.557894  0.261718 -0.930856 -0.044076   \n",
       "10433  0.266074  0.556689 -0.020434  0.176624  0.261718 -0.515608  0.597681   \n",
       "10434 -0.054866  0.580242  0.032912 -0.016668  0.261718  1.519109  0.371178   \n",
       "10435  0.080916  0.588093  0.015130  0.002250  0.261718 -0.930856 -0.270579   \n",
       "10436  0.377169  0.014957  0.381439  0.292753  0.261718 -1.470679 -0.006326   \n",
       "\n",
       "             F8        F9       F10 Class  \n",
       "0     -0.165983 -0.123005  1.087144     W  \n",
       "1      0.181844  2.090879 -2.009758     A  \n",
       "2     -0.450127  0.469443  0.060952     I  \n",
       "3      1.176165  0.968347 -0.627999     E  \n",
       "4     -0.253430 -0.497183  0.155681     A  \n",
       "...         ...       ...       ...   ...  \n",
       "10432  1.158458  2.277968 -0.699884     X  \n",
       "10433  0.178349  0.625350 -0.657245     G  \n",
       "10434 -0.985508 -0.403638  1.276301     A  \n",
       "10435  0.163807 -0.091823 -0.593329     F  \n",
       "10436 -0.494919 -0.247731 -1.212974     H  \n",
       "\n",
       "[10437 rows x 11 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As data has been already normalized we do not look into standardizing our data\n",
    "# As our data is already in numerical form we also don't look into cleaning our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since classes are imbalanced we could use class weights during learning\n",
    "# to provide some bias to minority classes without having to resample our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_train = df_train.drop(axis=1,labels=\"Class\");\n",
    "y_train = df_train['Class'];\n",
    "\n",
    "X_test = df_test.drop(axis=1,labels=\"Class\");\n",
    "y_test = df_test['Class'];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_labels = pd.get_dummies(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        1\n",
       "1        1\n",
       "2        1\n",
       "3        1\n",
       "4        0\n",
       "        ..\n",
       "10425    0\n",
       "10426    0\n",
       "10427    1\n",
       "10428    0\n",
       "10429    0\n",
       "Name: A, Length: 10430, dtype: uint8"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_labels['A']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Activation\n",
    "from keras import optimizers\n",
    "from keras.utils.vis_utils import plot_model\n",
    "np.random.seed(1);\n",
    "from keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'W', 'X', 'Y'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = y_train_labels.to_numpy();\n",
    "np.unique(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Maxime\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:71: FutureWarning: Pass classes=['A' 'B' 'C' 'D' 'E' 'F' 'G' 'H' 'I' 'W' 'X' 'Y'], y=0        A\n",
      "1        A\n",
      "2        A\n",
      "3        A\n",
      "4        F\n",
      "        ..\n",
      "10425    F\n",
      "10426    F\n",
      "10427    A\n",
      "10428    E\n",
      "10429    X\n",
      "Name: Class, Length: 10430, dtype: object as keyword args. From version 0.25 passing these as positional arguments will result in an error\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.utils import class_weight\n",
    "class_weight = class_weight.compute_class_weight('balanced', np.unique(y_train), y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  0.20279204, 173.83333333,   8.43851133,   2.46922348,\n",
       "         0.79375951,   0.44322625,   1.94880419,   1.67469493,\n",
       "         1.0459286 ,  19.75378788,   1.66507024,   3.26754386])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = y.astype('float32');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learning rate of 0.1 after multiple tries between 0.01 and 2\n",
    "# 3 couches cachés\n",
    "# 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "10430/10430 [==============================] - 1s 60us/step - loss: 1.3697 - accuracy: 0.5453\n",
      "Epoch 2/50\n",
      "10430/10430 [==============================] - 1s 51us/step - loss: 0.9982 - accuracy: 0.6528\n",
      "Epoch 3/50\n",
      "10430/10430 [==============================] - 1s 52us/step - loss: 0.9176 - accuracy: 0.6738\n",
      "Epoch 4/50\n",
      "10430/10430 [==============================] - 1s 51us/step - loss: 0.7925 - accuracy: 0.7032\n",
      "Epoch 5/50\n",
      "10430/10430 [==============================] - 1s 55us/step - loss: 0.7677 - accuracy: 0.7176\n",
      "Epoch 6/50\n",
      "10430/10430 [==============================] - 0s 47us/step - loss: 0.6887 - accuracy: 0.7376\n",
      "Epoch 7/50\n",
      "10430/10430 [==============================] - 0s 48us/step - loss: 0.6422 - accuracy: 0.7490\n",
      "Epoch 8/50\n",
      "10430/10430 [==============================] - 1s 59us/step - loss: 0.5958 - accuracy: 0.7690\n",
      "Epoch 9/50\n",
      "10430/10430 [==============================] - 1s 48us/step - loss: 0.6251 - accuracy: 0.7678\n",
      "Epoch 10/50\n",
      "10430/10430 [==============================] - 0s 43us/step - loss: 0.5968 - accuracy: 0.7715\n",
      "Epoch 11/50\n",
      "10430/10430 [==============================] - 0s 48us/step - loss: 0.5173 - accuracy: 0.7999\n",
      "Epoch 12/50\n",
      "10430/10430 [==============================] - 1s 48us/step - loss: 0.4755 - accuracy: 0.8131\n",
      "Epoch 13/50\n",
      "10430/10430 [==============================] - 0s 46us/step - loss: 0.4557 - accuracy: 0.8241\n",
      "Epoch 14/50\n",
      "10430/10430 [==============================] - 0s 48us/step - loss: 0.4263 - accuracy: 0.8329 0s - loss: 0.4329 - \n",
      "Epoch 15/50\n",
      "10430/10430 [==============================] - 0s 46us/step - loss: 0.4091 - accuracy: 0.8390\n",
      "Epoch 16/50\n",
      "10430/10430 [==============================] - 0s 48us/step - loss: 0.3891 - accuracy: 0.8477\n",
      "Epoch 17/50\n",
      "10430/10430 [==============================] - 1s 52us/step - loss: 0.3661 - accuracy: 0.8600\n",
      "Epoch 18/50\n",
      "10430/10430 [==============================] - 0s 47us/step - loss: 0.3359 - accuracy: 0.8667\n",
      "Epoch 19/50\n",
      "10430/10430 [==============================] - 0s 44us/step - loss: 0.3128 - accuracy: 0.8787\n",
      "Epoch 20/50\n",
      "10430/10430 [==============================] - 1s 52us/step - loss: 0.3078 - accuracy: 0.8814\n",
      "Epoch 21/50\n",
      "10430/10430 [==============================] - 0s 48us/step - loss: 0.2976 - accuracy: 0.8865\n",
      "Epoch 22/50\n",
      "10430/10430 [==============================] - 0s 45us/step - loss: 0.2836 - accuracy: 0.8945\n",
      "Epoch 23/50\n",
      "10430/10430 [==============================] - 1s 49us/step - loss: 0.2706 - accuracy: 0.8983\n",
      "Epoch 24/50\n",
      "10430/10430 [==============================] - 1s 49us/step - loss: 0.2710 - accuracy: 0.8994\n",
      "Epoch 25/50\n",
      "10430/10430 [==============================] - 1s 54us/step - loss: 0.2602 - accuracy: 0.9039\n",
      "Epoch 26/50\n",
      "10430/10430 [==============================] - 0s 46us/step - loss: 0.2250 - accuracy: 0.9157\n",
      "Epoch 27/50\n",
      "10430/10430 [==============================] - 0s 45us/step - loss: 0.2353 - accuracy: 0.9178\n",
      "Epoch 28/50\n",
      "10430/10430 [==============================] - 1s 49us/step - loss: 0.2062 - accuracy: 0.9228\n",
      "Epoch 29/50\n",
      "10430/10430 [==============================] - 0s 47us/step - loss: 0.1958 - accuracy: 0.9279\n",
      "Epoch 30/50\n",
      "10430/10430 [==============================] - 0s 45us/step - loss: 0.1928 - accuracy: 0.9293\n",
      "Epoch 31/50\n",
      "10430/10430 [==============================] - 0s 47us/step - loss: 0.1969 - accuracy: 0.9286\n",
      "Epoch 32/50\n",
      "10430/10430 [==============================] - 0s 47us/step - loss: 0.2055 - accuracy: 0.9346\n",
      "Epoch 33/50\n",
      "10430/10430 [==============================] - 0s 48us/step - loss: 0.1763 - accuracy: 0.9389\n",
      "Epoch 34/50\n",
      "10430/10430 [==============================] - 1s 50us/step - loss: 0.1566 - accuracy: 0.9438\n",
      "Epoch 35/50\n",
      "10430/10430 [==============================] - 0s 47us/step - loss: 0.1462 - accuracy: 0.9485\n",
      "Epoch 36/50\n",
      "10430/10430 [==============================] - 1s 48us/step - loss: 0.1444 - accuracy: 0.9478\n",
      "Epoch 37/50\n",
      "10430/10430 [==============================] - 1s 52us/step - loss: 0.1324 - accuracy: 0.9523\n",
      "Epoch 38/50\n",
      "10430/10430 [==============================] - 1s 48us/step - loss: 0.1366 - accuracy: 0.9512\n",
      "Epoch 39/50\n",
      "10430/10430 [==============================] - 1s 51us/step - loss: 0.1169 - accuracy: 0.9600\n",
      "Epoch 40/50\n",
      "10430/10430 [==============================] - 1s 54us/step - loss: 0.1008 - accuracy: 0.9652\n",
      "Epoch 41/50\n",
      "10430/10430 [==============================] - 1s 48us/step - loss: 0.0980 - accuracy: 0.9651\n",
      "Epoch 42/50\n",
      "10430/10430 [==============================] - 1s 49us/step - loss: 0.1333 - accuracy: 0.9592\n",
      "Epoch 43/50\n",
      "10430/10430 [==============================] - 1s 51us/step - loss: 0.0903 - accuracy: 0.9695\n",
      "Epoch 44/50\n",
      "10430/10430 [==============================] - 1s 48us/step - loss: 0.0890 - accuracy: 0.9709\n",
      "Epoch 45/50\n",
      "10430/10430 [==============================] - 1s 49us/step - loss: 0.1669 - accuracy: 0.9571\n",
      "Epoch 46/50\n",
      "10430/10430 [==============================] - 1s 50us/step - loss: 0.1040 - accuracy: 0.9660\n",
      "Epoch 47/50\n",
      "10430/10430 [==============================] - 1s 48us/step - loss: 1.1577 - accuracy: 0.7942\n",
      "Epoch 48/50\n",
      "10430/10430 [==============================] - 1s 51us/step - loss: 0.2749 - accuracy: 0.9021\n",
      "Epoch 49/50\n",
      "10430/10430 [==============================] - 1s 53us/step - loss: 0.2288 - accuracy: 0.9228\n",
      "Epoch 50/50\n",
      "10430/10430 [==============================] - 1s 53us/step - loss: 0.1709 - accuracy: 0.9404\n"
     ]
    }
   ],
   "source": [
    "n_h=200; n_o=12; lrate=0.1; nepochs=50; isVerbose=1;\n",
    "\n",
    "n_i = X_train.shape[1] # 10 distinct features in our data\n",
    "model = Sequential()\n",
    "model.add(Dense(n_h, input_dim=n_i, activation=\"relu\"));\n",
    "model.add(Dense(n_h, input_dim=n_h, activation=\"relu\"));\n",
    "model.add(Dense(n_h, input_dim=n_h, activation=\"relu\"));\n",
    "model.add(Dense(n_o, activation = \"softmax\"));\n",
    "\n",
    "model.compile(optimizer=optimizers.SGD(lr=lrate), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "history = model.fit(\n",
    "    X_train, \n",
    "    y, \n",
    "    epochs=nepochs, \n",
    "    batch_size=32,\n",
    "    verbose=isVerbose, \n",
    "    class_weight=class_weight,\n",
    "    #validation_split=0.4,\n",
    "    #callbacks=[EarlyStopping(monitor=\"val_loss\", patience=10, restore_best_weights=True)],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted = model.predict_classes(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert y_test to values\n",
    "y_test_valued = []\n",
    "for entry in y_test:\n",
    "    if (entry == 'A'):\n",
    "        y_test_valued.append(0);\n",
    "    elif (entry == 'B'):\n",
    "        y_test_valued.append(1);\n",
    "    elif (entry == 'C'):\n",
    "        y_test_valued.append(2);\n",
    "    elif (entry == 'D'):\n",
    "        y_test_valued.append(3);\n",
    "    elif (entry == 'E'):\n",
    "        y_test_valued.append(4);\n",
    "    elif (entry == 'F'):\n",
    "        y_test_valued.append(5);\n",
    "    elif (entry == 'G'):\n",
    "        y_test_valued.append(6);\n",
    "    elif (entry == 'H'):\n",
    "        y_test_valued.append(7);\n",
    "    elif (entry == 'I'):\n",
    "        y_test_valued.append(8);\n",
    "    elif (entry == 'W'):\n",
    "        y_test_valued.append(9);\n",
    "    elif (entry == 'X'):\n",
    "        y_test_valued.append(10);\n",
    "    elif (entry == 'Y'):\n",
    "        y_test_valued.append(11);\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testset Accuracy:  0.9251700680272109\n"
     ]
    }
   ],
   "source": [
    "count = 0;\n",
    "N = len(y_test_valued);\n",
    "for index, val in enumerate(y_test_valued):\n",
    "    if (predicted[index] == y_test_valued[index]):\n",
    "        count+=1;\n",
    "        \n",
    "print(\"Testset Accuracy: \", count / N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Even though we chose some very high large values for number of hidden layers\n",
    "# number of neurones, and a somewhat large number of epochs\n",
    "# We were still able to get a precision of 93+%\n",
    "# Which is why we could say that we did not overfit our sample."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
